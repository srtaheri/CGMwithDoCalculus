---
title: "Creating Training Data and True Parameters"
author: "Sara Taheri"
date: "6/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE}
library(usethis)
source("training_data_and_true_params.R", local = knitr::knit_global())
```

# TODO

Our overall contribution is to be using d-separation, do-calculus, Verma constraints to reason about the posterior distribution in a Bayesian hierarchical modeling framework. We are treating our Greek parameters as nodes on a DAG and using causal inference ideas to reason about them.In bayesian hiearachical modeling, we fit a model, look at the posterior, and reason about it. We look at visualizations of the posterior. We do posterior predictive checks.

1. Visualize dep/indp btw $(\alpha, \tau)$, $(\alpha, \eta)$, $(\mu, \tau)$, $(\eta, \mu)$ with/without hidden confounder

2. Generate histogram of $P(Y|do(X=x))$ for the ground truth, simulated without hidden confounder, and simulated with hidden confounder

   (a) Show that in the observed case the histogram centers round the true value.
   (b) Show that as D increases the histogram variance goes down
   (c) Show in the hidden case the histogram is biased
   (d) Show that increasing D does not fix the problem in the hidden case
   
Our HOPE is that if we constrain independence in training, we might get some improvements (I think it would be in 2.d, we are still biased but maybe have some consistency) But I don't know if that will work .

# Thread 1 : No mediator

We have a causal model that consists of three variables $U$, $X$, and $Y$ that have dimension of is $D \times L$, $D \times N$, and $D \times 1$ respectively. $U$ is a hidden confounder. Our causal model is as follows:


$$ U := N_{U} \\
X := U \alpha + N_{X} \\
Y := X \beta + U \eta + N_{Y}$$


Where $N_X$ and $N_y$ have a Normal distribution with mean zero and identity covariance matrix. Hence,

$$ U \sim N(\mu,\Sigma_{N_U N_U}) \\
X \sim N(\mu \alpha , \Sigma_{XX}) = N(\mu \alpha, \alpha^T \Sigma_{UU} \alpha + \Sigma_{N_X N_X})\\
Y \sim N(\mu \alpha \beta + \mu \eta, \Sigma_{YY}) = N(\mu \alpha \beta + \mu \eta, 
\beta^T \Sigma_{XX}\beta + \eta^T \Sigma_{UU} \eta + 2 \eta^T \Sigma_{UU} \alpha \beta + \Sigma_{N_Y N_Y}$$

For now we assume that $\Sigma_{N_U N_U}$, $\Sigma_{N_X N_X}$ and $\Sigma_{N_Y N_Y}$ are identity matrices. We assume that all the variables are observable for now. Let's look at the covariance matrix of all the variables:

$$\Sigma_{UU} = I_{UU}$$
Or we can estimate $\Sigma_{UU}$ from data for $U$. We can also estimate $\mu$ from mean of data for $U$. So $\mu$ and $\Sigma_{UU}$ can be identified uniquely.

$$\Sigma_{UX} = \alpha^T \Sigma_{UU} = \alpha^T$$
Since $\alpha^T$ is equal to $\Sigma_{UX}$, $\alpha$ can uniquely be identified. $\beta$ and $\eta$ should be derived from 3 remaining equations on $\Sigma_{UY}$, $\Sigma_{XY}$ and $\Sigma_{YY}$. Hence, they cannot be identified uniquely.

$$\Sigma_{UY} = (\beta^T \alpha^T + \eta^T ) \Sigma_{UU} = \beta^T \alpha^T + \eta^T$$

$$\Sigma_{XX} = \alpha^T \Sigma_{UU} \alpha + I_{N_X N_X}$$
$$\Sigma_{XY} = \Sigma_{XX} \beta + \alpha^T \Sigma_{UU} \eta$$
$$\Sigma_{YY} = \beta^T \Sigma_{XX}\beta + \eta^T \Sigma_{UU} \eta + 2 \eta^T \Sigma_{UU} \alpha \beta + \Sigma_{N_Y N_Y}$$

## Estimating the parameters in presence of hidden confounder U

If we intervene on variable $X$ and fix it's value at $x$,,

$$
P(Y | do(X = x)) \stackrel{d}{\sim} N(x \beta  + \mu \eta ,  \eta^T \Sigma_{UU} \eta + 1)
$$
We show that if we estimate the parameters of our causal generative model,  $\beta$ can uniquely recover from the distribution of observed data but $\mu$, $\alpha$ and $\eta$ can't. $\Sigma_{UXY}$ is the covariance matrix between all 3 variables,

$$
\Sigma_{UXY} = 
\begin{pmatrix}
\Sigma_{UU} & \Sigma_{UX}  & \Sigma_{UY}\\
\Sigma_{XU} & \Sigma_{XX}  & \Sigma_{XY}\\
\Sigma_{YU} & \Sigma_{YX}  & \Sigma_{YY}\\
\end{pmatrix} 
$$

The bottom right $2 \times 2$ sub-matrix represents observed variables. $\Sigma_{XX}$ is $N \times N$, $\Sigma_{XY}$ is $N \times 1$ and $\Sigma_{YY}$ is $1 \times 1$. The entries of this sub-matrix are as follows:

$$
\Sigma_{XX} = \alpha^T \Sigma_{UU} \alpha + \Sigma_{N_X N_X} \\
\Sigma_{XY} = \Sigma_{XX} \beta + \alpha^T \Sigma_{UU} \eta \\
\Sigma_{YY} = \beta^T \Sigma_{XX}\beta + \eta^T \Sigma_{UU} \eta + 2 \eta^T \Sigma_{UU} \alpha \beta + \Sigma_{N_Y N_Y}
$$
The number of equations in this model exceed the number of unknown parameters. Hence, there exists an equivalence class of structural equations with parameters 

$$
(\mu_1,\alpha_1,\Sigma_{UU,1},\beta_1,\eta_1) \neq (\mu, \alpha,\Sigma_{UU},\beta,\eta)
$$
that return the same observable covariance matrix. If we replace $\alpha$ and $\Sigma_{UU}$ with,
$\alpha_1 := c.\alpha$ and 
$\Sigma_{UU,1} = \Sigma_{UU}/{c^2}$ respectively, where $c$ is a non-zero constant, then the expression for $\Sigma_{XX}$ in Equation stays the same. However, if we change $\beta$ to any different value, Equation $\Sigma_{XY}$ does not hold true, so $\beta$ has to stay unchanged. If we set $\eta_1 = c.\eta$, then $\Sigma_{XY}$ stays unchanged. With the same setting for $\alpha$, $\beta$ and $\eta$, $\Sigma_{YY}$ stays unchanged. $\mu$ does not appear in these equations because it only affects the mean.

Finally, we conclude that there is an uncertainty region over $\mu$, $\alpha$, $\eta$ and $\Sigma_{UU}$ , but $\beta$ and $\tau$ will be identified uniquely. We showed these results in ```unconstrained_hmc.Rmd```, ```unconstrained_svi.Rmd``` and ```MLE.Rmd```.

## Dependence and independence relationships between parameters

```{r, include=FALSE}
library(bnlearn)
library(tidyverse)
net <- model2network("[alpha][beta][eta][mu][U|mu][X|U:alpha][Y|X:U:beta:eta]")
# U is observed.
posterior_set_obs <- c("alpha","beta","eta","mu")
conditioning_set_obs <- c('X', 'Y', 'U')
test_dsep_obs <- function(.x, .y) dsep(net, .x, .y, conditioning_set_obs)
# U is hidden.
posterior_set_hid <- c("alpha","beta","eta","mu",'U')
conditioning_set_hid <- c('X', 'Y')
test_dsep_hid <- function(.x, .y) dsep(net, .x, .y, conditioning_set_hid)

dep_indp <- data.frame("parameter1" = 0, "parameter2" = 0, "observed_ind" = 0, "hidden_ind" = 0)
for (i in 1:(length(posterior_set_obs)-1)) {
  for (j in (i+1):(length(posterior_set_obs))) {
    dep_indp = rbind(dep_indp,
                     data.frame("parameter1" = posterior_set_obs[i],
                                "parameter2" = posterior_set_obs[j],
                                "observed_ind" = test_dsep_obs(.x = posterior_set_obs[i], .y = posterior_set_obs[j]),
                                "hidden_ind" = test_dsep_hid(posterior_set_obs[i], posterior_set_obs[j])
                                )
                     )
  }
}
dep_indp <- dep_indp[-1,]
```

```{r}
dep_indp
```
Except for the relationship between (beta, eta), other pairwise relationships which were independent when all the variables are observed, they become dependent.

# Thread 2 : With mediator 

We have a causal model that consists of four variables $U$, $X$, $M$ and $Y$ that have dimension of  $D \times L$, $D \times N$, $D \times 1$, and $D \times 1$ respectively. $U$ is a hidden confounder. Our causal model is as follows:


$$ U := N_{U} \\
X := U \alpha + N_{X} \\
M := X \beta + N_{M} \\
Y := M \tau + U \eta + N_{Y}$$

Where $N_X$, $N_M$, and $N_y$ have a Normal distribution with mean zero and identity covariance matrix. Hence,

$$ U \sim N(\mu,\Sigma_{UU}) \\
X \sim N(\mu \alpha , \Sigma_{XX}) = N(\mu \alpha, \alpha^T \Sigma_{UU} \alpha + \Sigma_{N_X N_X})\\
M \sim N(\mu \alpha \beta, \Sigma_{MM}) = N(\mu \alpha \beta,\beta^T \Sigma_{XX} \beta + \Sigma_{N_M N_M})\\
Y \sim N(\mu \alpha \beta \tau + \mu \eta, \Sigma_{YY}) = N(\mu \alpha \beta \tau + \mu \eta,\tau^T \Sigma_{MM} \tau + 2 \eta^T \Sigma_{UU} \alpha \beta \tau  + \eta^T \Sigma_{uu} \eta + \Sigma_{N_Y N_Y})$$

The dimensions of $\mu$, $\alpha$, $\beta$, $\tau$, and $\eta$ are $1 \times L$, $L \times N$, $N \times 1$, $1 \times 1$, and $L \times 1$ respectively. L is the total number of latent variables, N is the total number of causes and D is the total number of data points.


If we intervene on variable $X$ and fix it's value at $x$,,

$$
P(Y | do(X = x)) \stackrel{d}{\sim} N(x \beta \tau + \mu \eta ,  \tau^T \tau + \eta^T \Sigma_{UU} \eta + 1)
$$
We show that if we train the parameters of our causal generative model,  $\beta$ and $\tau$ can uniquely recover from the distribution of observed data.

## Estimating the parameters when there is no hidden confounder

If there is no hidden confounder, the posterior over the parameters is $P(\alpha, \beta, \tau, \mu, \eta | X, M, Y, U)$. In this case, the dependence and independence relationships between the parameters is as follows:

```{r, include=FALSE}
library(bnlearn)
library(tidyverse)
net <- model2network("[alpha][beta][tau][eta][mu][U|mu][X|U:alpha][M|X:beta][Y|eta:tau:M:U]")
# U is observed.
posterior_set_obs <- c("alpha","beta","tau","eta","mu")
conditioning_set_obs <- c('X', 'Y', 'M', 'U')
test_dsep_obs <- function(.x, .y) dsep(net, .x, .y, conditioning_set_obs)
# U is hidden.
posterior_set_hid <- c("alpha","beta","tau","eta","mu",'U')
conditioning_set_hid <- c('X', 'Y', 'M')
test_dsep_hid <- function(.x, .y) dsep(net, .x, .y, conditioning_set_hid)

dep_indp <- data.frame("parameter1" = 0, "parameter2" = 0, "observed_ind" = 0, "hidden_ind" = 0)
for (i in 1:(length(posterior_set_obs)-1)) {
  for (j in (i+1):(length(posterior_set_obs))) {
    dep_indp = rbind(dep_indp,
                     data.frame("parameter1" = posterior_set_obs[i],
                                "parameter2" = posterior_set_obs[j],
                                "observed_ind" = test_dsep_obs(.x = posterior_set_obs[i], .y = posterior_set_obs[j]),
                                "hidden_ind" = test_dsep_hid(posterior_set_obs[i], posterior_set_obs[j])
                                )
                     )
  }
}
dep_indp <- dep_indp[-1,]
```

```{r}
dep_indp
```


# Estimating the parameters in presence of hidden confounder U

If U is hidden, then all the dependence and independence relationships between the parameters will stay the same as the case when there is no hidden confounder except for the following pairwise relationships:

```{r}
dep_indp[c(2,3,4,9,10),]
```

This table shows that these pairwise relationships suddenly become dependent when U is hidden. We want to show how we can train our model to break the dependence between  $\alpha$ and $\eta$.


# P(Y | do(X = x))
If we intervene on variable $X$ and fix it's value at $x$,,

$$
P(Y | do(X = x)) \stackrel{d}{\sim} N(x \beta \tau + \mu \eta ,  \tau^T \tau + \eta^T \Sigma_{UU} \eta + 1)
$$
We show that if we estimate the parameters of our causal generative model,  $\beta$ and $\tau$ can uniquely recover from the distribution of observed data. $\Sigma_{UXMY}$ is the covariance matrix between all 4 variables,

$$
\Sigma_{UXMY} = 
\begin{pmatrix}
\Sigma_{UU} & \Sigma_{UX} & \Sigma_{UM} & \Sigma_{UY}\\
\Sigma_{XU} & \Sigma_{XX} & \Sigma_{XM} & \Sigma_{XY}\\
\Sigma_{MU} & \Sigma_{MX} & \Sigma_{MM} & \Sigma_{MY}\\
\Sigma_{YU} & \Sigma_{YX} & \Sigma_{YM} & \Sigma_{YY}\\
\end{pmatrix} 
$$
The bottom right $3 \times 3$ sub-matrix represents observed variables. $\Sigma_{XX}$ is $N \times N$, $\Sigma_{XM}$ is $N \times 1$, $\Sigma_{XY}$ is $N \times 1$ and the rest are $1 \times 1$. The entries of this sub-matrix are as follows:

$$
\Sigma_{XX} = \alpha^T \Sigma_{UU} \alpha + \Sigma_{N_X N_X} \\
\Sigma_{XM} = \Sigma_{XX} \beta \\
\Sigma_{XY} = \Sigma_{XX} \beta \tau + \alpha^T \Sigma_{UU} \eta \\
\Sigma_{MM} = \beta^T \Sigma_{XX} \beta + 1\\
\Sigma_{MY} = \beta^T \Sigma_{XX} \beta \tau + \beta^T \alpha^T \Sigma_{UU} \eta  + \Sigma_{N_M N_M} \tau \\
\Sigma_{YY} = \tau^T \Sigma_{MM} \tau + 2 \eta^T \Sigma_{UU} \alpha \beta \tau  + \eta^T \Sigma_{UU} \eta + \Sigma_{N_Y N_Y}
$$
The number of equations in this model exceed the number of unknown parameters. Hence, there exists an equivalence class of structural equations with parameters

$$
(\mu_1,\alpha_1,\Sigma_{UU,1},\beta_1,\tau_1,\eta_1) \neq (\mu, \alpha,\Sigma_{UU},\beta,\tau,\eta)
$$
that return the same observable covariance matrix. If we replace $\alpha$ and $\Sigma_{UU}$ with,
$\alpha_1 := c.\alpha$ and 
$\Sigma_{UU,1} = \Sigma_{UU}/{c^2}$ respectively, where $c$ is a non-zero constant, then the expression for $\Sigma_{XX}$ in Equation stays the same. However, if we change $\beta$ to any different value, Equation $\Sigma_{XM}$ does not hold true, so $\beta$ has to stay unchanged. If we set $\eta_1 = c.\eta$, then $\Sigma_{XY}$ stays unchanged and since we concluded that $\beta$ should stay unchanged, there is no other value that $\tau$ could take such that the equation that calculates $\Sigma_{XY}$ holds true.

Now, let's look at the expected value of each observed variable:

$$
E[X] = \mu \alpha \\
E[M] = \mu \alpha \beta \\
E[Y] = \mu \alpha \beta \tau + \mu \eta
$$

If $\mu_1 := \mu/c$, then, given that $\alpha_1 := c.\alpha$, $\beta_1 := \beta$, $\tau_1 := \tau$, and $\eta_1 := c.\eta$, all the equations above hold true.

Finally, we conclude that there is an uncertainty region over $\mu$, $\alpha$, $\eta$ and $\Sigma_{UU}$ , but $\beta$ and $\tau$ will be identified uniquely. We showed these results in ```unconstrained_hmc.Rmd```, ```unconstrained_svi.Rmd``` and ```MLE.Rmd```.
